{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3677cc0",
   "metadata": {},
   "source": [
    "说明：Inception Moudel\n",
    "\n",
    "1、卷积核超参数选择困难，自动找到卷积的最佳组合。\n",
    "\n",
    "2、1x1卷积核，不同通道的信息融合。使用1x1卷积核虽然参数量增加了，但是能够显著的降低计算量(operations)\n",
    "\n",
    "3、Inception Moudel由4个分支组成，要分清哪些是在Init里定义，哪些是在forward里调用。4个分支在dim=1(channels)上进行concatenate。24+16+24+24 = 88\n",
    "\n",
    "4、GoogleNet的Inception(Pytorch实现):https://www.cnblogs.com/Mrzhang3389/p/10127157.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f1afd",
   "metadata": {},
   "source": [
    "---\n",
    "代码说明：  \n",
    "1、先使用类对Inception Moudel进行封装\n",
    "\n",
    "2、先是1个卷积层(conv,maxpooling,relu)，然后inceptionA模块(输出的channels是24+16+24+24=88)，接下来又是一个卷积层(conv,mp,relu),然后inceptionA模块，最后一个全连接层(fc)。\n",
    "\n",
    "3、1408这个数据可以通过x = x.view(in_size, -1)后调用x.shape得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c615c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.949\n",
      "[1,   600] loss: 0.202\n",
      "[1,   900] loss: 0.150\n",
      "accuracy on test set: 96 % \n",
      "[2,   300] loss: 0.119\n",
      "[2,   600] loss: 0.106\n",
      "[2,   900] loss: 0.094\n",
      "accuracy on test set: 97 % \n",
      "[3,   300] loss: 0.085\n",
      "[3,   600] loss: 0.082\n",
      "[3,   900] loss: 0.077\n",
      "accuracy on test set: 98 % \n",
      "[4,   300] loss: 0.069\n",
      "[4,   600] loss: 0.071\n",
      "[4,   900] loss: 0.065\n",
      "accuracy on test set: 98 % \n",
      "[5,   300] loss: 0.063\n",
      "[5,   600] loss: 0.061\n",
      "[5,   900] loss: 0.054\n",
      "accuracy on test set: 98 % \n",
      "[6,   300] loss: 0.051\n",
      "[6,   600] loss: 0.054\n",
      "[6,   900] loss: 0.049\n",
      "accuracy on test set: 98 % \n",
      "[7,   300] loss: 0.047\n",
      "[7,   600] loss: 0.046\n",
      "[7,   900] loss: 0.046\n",
      "accuracy on test set: 98 % \n",
      "[8,   300] loss: 0.043\n",
      "[8,   600] loss: 0.039\n",
      "[8,   900] loss: 0.046\n",
      "accuracy on test set: 98 % \n",
      "[9,   300] loss: 0.041\n",
      "[9,   600] loss: 0.039\n",
      "[9,   900] loss: 0.039\n",
      "accuracy on test set: 98 % \n",
      "[10,   300] loss: 0.033\n",
      "[10,   600] loss: 0.039\n",
      "[10,   900] loss: 0.038\n",
      "accuracy on test set: 98 % \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "# prepare dataset\n",
    " \n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差--->(1,28,28)\n",
    " \n",
    "train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    " \n",
    "# design model using class\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        # 链路1 in_channels --> out_channels(16),1*1卷积核,不需要padding就能保持图像宽和高不变\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        \n",
    "        # 链路2\n",
    "        #in_channels --> out_channels(16),1*1卷积核，不需要padding就能保持图像宽和高不变\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        #in_channels（16） --> out_channels(24),5*5卷积核，padding(只在一个channel内操作)为2（卷积核计算得到5/2=2）是为了保持图像宽度和高度不变\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "        \n",
    "        # 链路3\n",
    "        #in_channels --> out_channels(16),1*1卷积核，不需要padding就能保持图像宽和高不变\n",
    "        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        #in_channels（16） --> out_channels(24),3*3卷积核，padding(只在一个channel内操作)为1（卷积核计算得到3/2=1）是为了保持图像宽度和高度不变\n",
    "        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        #in_channels（24） --> out_channels(24),3*3卷积核，padding(只在一个channel内操作)为1（卷积核计算得到3/2=1）是为了保持图像宽度和高度不变\n",
    "        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 链路4\n",
    "        #in_channels --> out_channels(24),1*1卷积核，不需要padding就能保持图像宽和高不变\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # 链路1\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        # 链路2\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        # 链路3\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "        branch3x3 = self.branch3x3_3(branch3x3)\n",
    "        # 链路4\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        # 4个链路拼接,outputs chneels(24*3+16=88)，拼接的前提是4个链路输出的维度中（batch,channel,width,high）中（b,w,h）保持一样，\n",
    "        #在c的维度上进行拼接\n",
    "        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "        return torch.cat(outputs, dim=1) # b,c,w,h  c对应的是dim=1\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5) # 88 = 24x3 + 16\n",
    " \n",
    "        self.incep1 = InceptionA(in_channels=10) # 与conv1 中的10对应\n",
    "        self.incep2 = InceptionA(in_channels=20) # 与conv2 中的20对应\n",
    " \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10) \n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))#\n",
    "        x = self.incep1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    " \n",
    "        return x\n",
    " \n",
    "model = Net()\n",
    " \n",
    "# construct loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "# training cycle forward, backward, update\n",
    " \n",
    " \n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 获得一个批次的数据和标签\n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()\n",
    "        ## forward\n",
    "        # 获得模型预测结果\n",
    "        outputs = model(inputs)\n",
    "        # 交叉熵代价函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss = 0.0\n",
    " \n",
    " \n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('accuracy on test set: %d %% ' % (100*correct/total))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daa7ef",
   "metadata": {},
   "source": [
    "---\n",
    "说明：   \n",
    "\n",
    "1、要解决的问题：梯度消失\n",
    "\n",
    "2、跳连接，H(x) = F(x) + x,张量维度必须一样，加完后再激活。不要做pooling，张量的维度会发生变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9be1e8",
   "metadata": {},
   "source": [
    "---\n",
    "代码说明：\n",
    "\n",
    "1、先是1个卷积层(conv,maxpooling,relu)，然后ResidualBlock模块，接下来又是一个卷积层(conv,mp,relu),然后esidualBlock模块模块，最后一个全连接层(fc)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d5a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.529\n",
      "[1,   600] loss: 0.159\n",
      "[1,   900] loss: 0.114\n",
      "accuracy on test set: 97 % \n",
      "[2,   300] loss: 0.094\n",
      "[2,   600] loss: 0.081\n",
      "[2,   900] loss: 0.071\n",
      "accuracy on test set: 98 % \n",
      "[3,   300] loss: 0.061\n",
      "[3,   600] loss: 0.061\n",
      "[3,   900] loss: 0.059\n",
      "accuracy on test set: 98 % \n",
      "[4,   300] loss: 0.047\n",
      "[4,   600] loss: 0.054\n",
      "[4,   900] loss: 0.048\n",
      "accuracy on test set: 98 % \n",
      "[5,   300] loss: 0.041\n",
      "[5,   600] loss: 0.042\n",
      "[5,   900] loss: 0.040\n",
      "accuracy on test set: 98 % \n",
      "[6,   300] loss: 0.037\n",
      "[6,   600] loss: 0.036\n",
      "[6,   900] loss: 0.036\n",
      "accuracy on test set: 98 % \n",
      "[7,   300] loss: 0.031\n",
      "[7,   600] loss: 0.032\n",
      "[7,   900] loss: 0.032\n",
      "accuracy on test set: 98 % \n",
      "[8,   300] loss: 0.027\n",
      "[8,   600] loss: 0.030\n",
      "[8,   900] loss: 0.027\n",
      "accuracy on test set: 98 % \n",
      "[9,   300] loss: 0.026\n",
      "[9,   600] loss: 0.025\n",
      "[9,   900] loss: 0.025\n",
      "accuracy on test set: 98 % \n",
      "[10,   300] loss: 0.022\n",
      "[10,   600] loss: 0.021\n",
      "[10,   900] loss: 0.025\n",
      "accuracy on test set: 98 % \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "# prepare dataset\n",
    " \n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差\n",
    " \n",
    "train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    " \n",
    "# design model using class\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        #ResidualBlock的 输入输出channel 一样才能最后和输入相加\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)#图像宽和高保持不变\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x + y)\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5) # 88 = 24x3 + 16\n",
    " \n",
    "        self.rblock1 = ResidualBlock(16)\n",
    "        self.rblock2 = ResidualBlock(32)\n",
    " \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(512, 10) # 暂时不知道512咋能自动出来的\n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    " \n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "        x = self.rblock2(x)\n",
    " \n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    " \n",
    "model = Net()\n",
    " \n",
    "# construct loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "# training cycle forward, backward, update\n",
    " \n",
    " \n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss = 0.0\n",
    " \n",
    " \n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('accuracy on test set: %d %% ' % (100*correct/total))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b258810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
